\chapter{Revisão Bibliográfica}\label{chap3}

Este capítulo apresenta o levantamento bibliográfico efetuado ao longo do trabalho. Os artigos apresentados foram divididos em dois tópicos principais: estratégias de escalonamento de tarefas de \textit{workflows} em 
ambientes distribuídos (Seção \ref{sec3:ws});  e estratégias de escalonamento com alocação de dados (Seção \ref{sec3:tds}).  

\section{Estratégias de Escalonamento de tarefas de \textit{workflows} em Ambientes Distribuídos}\label{sec3:ws}

Os algoritmos de escalonamento podem ser divididos em dois grupos de interesse \cite{Smanchat}: algoritmos direcionados ao usuário e algoritmos direcionados ao provedor. A principal diferença entre esses grupos é em relação aos objetivos do escalonamento, isto é, enquanto os algoritmos do lado provedor concentram-se principalmente nas camadas físicas, como distribuição da carga entre os \textit{data centers} e a minimização dos gastos energéticos, os orientados aos usuários concentram-se nos atributos relacionados a contratação dos serviços, como o tempo de execução das aplicações e os custos monetários associados ao uso dos recursos. Como a proposta deste trabalho é voltada aos interesses dos cientistas, este levantamento bibliográfico aborda apenas as soluções de escalonamento voltadas aos usuários. Para auxiliar na leitura, um resumo dos trabalhos discutidos é apresentado na Tabela \ref{tab:trabalhos1}.


\subsection{Heurísticas}

Segundo o levantamento feito em \cite{Masdari16}, a maioria das propostas de escalonamento de \textit{workflows} são baseadas em heurísticas. Neste contexto, uma das heurísticas mais utilizadas é o \textit{Heterogeneous Earliest-Finish-Time} (HEFT) \cite{HEFT}. O HEFT é uma extensão   para ambientes heterogêneos dos algoritmos clássicos de \textit{list scheduling} \cite{SCHUTTEN96}. O algoritmo executa o escalonamento em duas fases: \textit{fase de ranqueamento}, que calcula a prioridade de cada tarefa; e \textit{fase de seleção de máquina}, que escolhe uma tarefa de acordo com o valor de prioridade, e a atribui para a máquina na qual seu tempo de execução será o menor. Inúmeras variações do HEFT foram propostas para o escalonamento de WfCs \cite{Yu07,chopra13, durillo12, Durillo14}. Em \cite{Yu07} um algoritmo de escalonamento adaptativo para ambientes de \textit{grids} é apresentado. O algoritmo, chamado de AHEFT, utiliza o escalonamento definido pelo HEFT e executa reescalonamentos caso sejam detectadas mudanças no ambiente, como a adição ou remoção de máquinas. O objetivo do AHEFT é minimizar o \textit{makespan}. Em Chopra e Singh \cite{chopra13}, o escalonamento dado pelo HEFT é também combinado a técnicas de reescalonamento. Porém, diferente de \cite{Yu07}, os autores executam o escalonamento de \textit{workflows} em ambientes híbridos, que combinam nuvens públicas e privadas. O HEFT é utilizado na execução inicial (realizada na nuvem privada), e caso o \textit{makespan} resultante seja maior que o tempo máximo de execução definido pelo usuário, a heurística seleciona um conjunto de tarefas para serem processadas na nuvem pública. 

Em Durillo, Fard e Prodan \cite{durillo12} o \textit{Multi-Objective} HEFT (MOHEFT) é proposto. O MOHEFT computa um conjunto de soluções, chamadas de Fronteira de Pareto. Essas soluções apresentam uma condição de balanceamento, na qual a melhora de um dos objetivos implica na piora de outra. Em \cite{durillo12}, o MOHEFT foi aplicado a ambientes de nuvens públicas, mais especificamente nos serviços da Amazon EC2. O objetivo era minimizar o \textit{makespan} e diminuir os custos financeiros. Em outro trabalho \cite{Durillo14}, o MOHEFT foi avaliado em \textit{clusters} heterogêneos compostos por 100 nós, o objetivo do trabalho foi minimizar o gasto energético e o \textit{makespan}. Em ambas as avaliações o MOHEFT apresentou soluções viáveis e resultados melhores que heurísticas mais simples, como o próprio HEFT por exemplo. Embora os algoritmos baseadas no HEFT considerem aspectos relacionados ao ambiente, como a capacidade de processamento das máquinas e as taxas de transferências, as restrições de armazenamento não são consideradas. Além disso, os algoritmos também não levam em conta a localização dos dados para decidir o escalonamento das tarefas.

O algoritmo guloso MinMin \cite{minmin} também foi utilizado no escalonamento de \textit{workflows}. O algoritmo escalona as tarefas prontas para serem executadas com base nas informações locais, como tempo de execução das tarefas e a capacidade de processamento das máquinas. A cada iteração, o tempo de término de cada tarefa em relação as máquinas disponíveis é estimado, e a tarefa que apresentar o menor tempo é escalonada primeiro. Essas etapas são repetidas até que não haja mais tarefas para serem escalonadas. Segundo Blythe \textit{et al.} \cite{minmin} a ideia por trás da heurística é  garantir que o tempo total de execução seja incrementado aos poucos, de modo que o \textit{makespan} resultante seja minimizado. Porém, como a heurística não considera o grafo na decisão de escalonamento e o modelo de ambiente considera apenas o poder de processamento das máquinas e as taxas de transferências, a solução resultante pode ser de baixa qualidade e até mesmo inviável, caso haja restrições de armazenamento.

Heurísticas baseadas em particionamento de grafo também foram propostas para este problema \cite{Byun2011, Abrishami}. A ideia geral dos algoritmos de particionamento é definir e alocar subgrafos, de modo que restrições e objetivos sejam satisfeitos. Byun \textit{et al.} \cite{Byun2011} apresentou a heurística PBTS (\textit{Partitioned Balanced Time Scheduling}), cujo objetivo é minimizar o  tempo de execução, respeitando um tempo limite definido pelo usuário. O algoritmo constrói o escalonamento iterativamente, com base no período mínimo de contratação das MVs, que é definido pelo provedor da nuvem. No Amazon EC2, por exemplo, esse período é de 60 minutos, ou seja, se a aplicação for executada por 61 minutos, o usuário é cobrado por 2 períodos (120 minutos). O PBTS define, para cada período, o número mínimo de MVs suficiente para executar as tarefas de uma partição. Nessa abordagem, o usuário define um tempo limite de execução para todo o \textit{workflow}, que é então dividido em sub-limites e atribuído para cada partição (subgrafos do \textit{workflow}). O algoritmo considera os tempos de execução das tarefas e de transferência dos dados, e assume que a comunicação entre tarefas é feita exclusivamente via o Amazon S3 (\url{https://aws.amazon.com/pt/s3/}), que é um serviço de armazenamento compartilhado oferecido pela Amazon \cite{AmazonEC2}. Embora o uso do Amazon S3 facilite as trocas de dados entre as tarefas do \textit{workflow}, as alocações individuais dos arquivos entre as máquinas não podem ser exploradas. Além disso, conforme demonstrado por Juve \textit{et al.} \cite{juve2010}, o Amazon S3 apresenta uma baixa performance para \textit{workflows} compostos por pequenos arquivos, o que é comum em WfCs.

A heurística IC-PCP (\textit{IaaS Cloud Partial Critical Paths}) \cite{Abrishami} constrói subconjuntos de tarefas com base nos caminhos críticos do \textit{workflow}. Cada um desses subconjuntos é escalonado para uma MV, escolhida conforme seu valor financeiro e sua capacidade de processamento. Durante a escolha das máquinas virtuais, o algoritmo dá preferencia às máquinas que já estão em uso. Caso seja necessário alocar novas máquinas, a MV mais barata com capacidade de processamento suficiente para executar o subgrupo de tarefas dentro do tempo limite de execução é escolhida. A busca por caminhos críticos e todo o processo de escalonamento é repetido até que não haja tarefas para serem escalonadas. Embora o agrupamento de tarefas seja capaz de reduzir os custos de transferências, a localização dos dados não é implicitamente explorado por essa abordagem.


\subsection{Metaheurísticas}

As metaheurísticas foram largamente empregadas no escalonamento de WfCs em diferentes ambientes distribuídos. Entre as principais metaheurísticas adotadas destacam-se as baseadas em população, tais como, \textit{Particle Swarm Optimization} (PSO) \cite{kennedy95}, \textit{Ant Colony Optinization} (ACO) \cite{dorigo99} e Algoritmos Genéticos (AG) \cite{goldberg1989}.

O PSO é uma metaheurística baseada no comportamento social de animais, como por exemplo um bando de pássaros buscando comida ou um cardume protegendo-se de predadores. No PSO, o movimento das chamadas partículas é análogo ao "caminhar" de um indivíduo sobre o espaço de busca e sua posição, em um dado intervalo de tempo, é baseada na melhor posição conhecida e na posição da melhor partícula do grupo \cite{pandey2010}. Pandey \textit{et al.} \cite{pandey2010} utilizam o PSO como parte de uma heurística dinâmica de escalonamento, cujo objetivo é minimizar o custo financeiro de \textit{workflows} executados em ambientes de nuvens públicas. A abordagem proposta foi capaz de balancear dinamicamente a carga de tarefas entre os recursos disponíveis e apresentou resultados três vezes melhor em relação a heurística avaliada. Em Rodriguez e Buyya \cite{Rodriguez2014} é apresentado um PSO para o escalonamento estático de WfCs em ambientes de nuvem, cujo objetivo é minimizar o custo financeiro atendendo à restrição de tempo de execução imposta pelo usuário. Diferente de \cite{pandey2010}, que considera recursos homogêneos, a solução de \cite{Rodriguez2014} define as MVs que serão contratadas, o período de contratação e o plano de escalonamento. Embora ambos considerem os tempos de transferências dos arquivos na decisão de escalonamento, a distribuição dos dados no ambiente não é levada em consideração pelos escalonadores.


O ACO é uma metaheurística inspirada no comportamento cooperativo desempenhado pelas formigas durante a busca de alimentos \cite{dorigo99}. Essa metaheurística tem sido aplicada com sucesso em vários problemas reais, inclusive em problemas de otimização combinatória. Chen e  Zhang \cite{Chen2009} apresentam uma solução de escalonamento multi-objetivo para \textit{workflows} executados em \textit{grids}. Os autores propõem uma metaheurística baseada no ACO para atender a três dos principais parâmetros de QoS: confiabilidade dos serviços; custo financeiro; e
\textit{makespan}. São definidas três classes de otimização: otimização de confiabilidade; otimização de \textit{makespan}; e otimização de custo. Essas classes buscam maximizar (ou minimizar, no caso do \textit{makespan}) uma métrica de QoS específica e atender as restrições impostas pelas demais métricas. Para a atualização do feromônio, os autores propuseram sete diferentes heurísticas que são selecionadas em tempo de execução, conforme um esquema adaptativo de própria autoria. O algoritmo obteve em média valores de custo financeiro entre 20\% e 30\% menores do que as heurísticas avaliadas. Em \cite{Hu2010}  o  algoritmo \textit{knowledge-based ant colony optimization} (KBACO) é apresentada. O KBACO constrói escalonamentos estáticos para \textit{workflows} em \textit{grids} utilizando a metaheurística ACO juntamente com heurísticas que utilizam o aprendizado acumulado a cada etapa da construção para melhorar as decisões de escalonamento. O escalonamento deve garantir que um limite de tempo de execução imposto pelo usuário ao \textit{workflow} seja atendido. Ambas as abordagens não consideram as características da rede (taxa de transferência, por exemplo) na decisão de escalonamento.  

Yu e Buyya \cite{Yu2006} apresentaram um GA para o problema de escalonamento estático de \textit{workflows} para ambientes de nuvem.
Nessa proposta, o usuário pode definir qual restrição (tempo máximo de execução ou custo monetário máximo) a metaheurística deverá atender. A solução é representada por uma codificação bidimensional (máquinas x tarefas), e os métodos \textit{Best-fit} e \textit{Round-Robin} são empregados para gerar a população inicial. As abordagens clássicas de  troca e recombinação em dois pontos são empregadas no operador de mutação e no de \textit{crossover}, respectivamente. 


\section{Estratégias de Escalonamento de Tarefas e Alocação de Dados para WfCs}\label{sec3:tds}

Alguns trabalhos consideraram o impacto causado pelas transferência de dados no tempo de execução dos \textit{workflow}, outros, chamados de \textit{data-aware}, executam o escalonamento dos dados e das tarefas, de forma separada.

Em Szabo \textit{et al.} \cite{Szabo2013}, o impacto das transferências de dados no escalonamento de WfCs é discutido. Os autores argumentam que por conta do aumento no volume de dados transferidos entre as tarefas de um \textit{workflow}, as soluções de escalonamento devem considerar a relação entre dados e tarefas para definir o plano de escalonamento. Os autores apresentaram um algoritmo evolutivo que otimiza o escalonamento de tarefas tendo em vista a diminuição das transferências  entre essas e a minimização do tempo de execução do \textit{workflow}. Nesse trabalho, um modelo de transferência e de armazenamento de dados baseado no Amazon S3 é empregado. Além disso, a distribuição dos dados é definida conforme a  alocação das tarefas, isto é, os arquivos de saída das tarefas são escritos tanto no S3, quanto na máquina associada a tarefa. Como resultado, apenas as tarefas alocadas para essa mesma máquina tiram vantagem da alocação dos arquivos, sendo que as outras tarefas devem fazer o \textit{download} diretamente do S3. 

% In  Szabo \textit{et al.} \cite{Szabo2013}, the impact of data transfer in the scheduling of data-intensive scientific workflows is discussed. The authors argue that since the volume of data transferred between tasks of workflows have increased a lot,  the  data  and  task assignment problems  can not be treated independently. The authors propose an evolutionary algorithm that optimizes the task scheduling aiming at minimizing the data transfer among tasks and  the total execution time of the workflow. 

% The metaheuristic proposed by Szabo \textit{et al.} uses a storage and data transfer model based on Amazon S3. Moreover, the file  allocation is driven only by the task assignment, i.e.,  the  output file is  written both in the S3 and in the machine  where the task that generated it is assigned. As a result, only tasks that were allocated to the same machine can take advantage of that file allocation.   Others tasks have to download the file  from the shared storage S3. In our work the allocation of tasks and files considers, among other things, the distribution of all tasks and files in the virtual environment.
% %
% Although it was our intention to compare the results of Szabo \textit{et al.} with ours, the description of the approach given in Szabo's paper did not allow us 
% %to replicate their experiments
% to reproduce the presented results in terms of quality of solutions (the source code was not available either). 
% Thus, in order to present a fair comparison in our paper, we have opted to drop the comparison with Szabo's work and  compare with traditional scheduling approaches such as HEFT \cite{HEFT} and Min-Min \cite{MINMIN}.


Yuan \textit{et al.} \cite{Yuan2010} utiliza uma técnica de clusterização \cite{Broder} baseada em uma matriz de dependência para identificar os arquivos que serão consumidos (isto é, compartilhados) pelas mesmas tarefas do \textit{workflow}. A ideia é armazenar cada arquivo ao \textit{data-center} que contenha o maior número de tarefas que depende deles e, dessa forma, reduzir as transferências entre \textit{data-centers}. O problema de alocação de tarefas e a distribuição dos arquivos não é profundamente explorado pela pelo algoritmo de Yuan \textit{et al.}, pois os escalonamentos de tarefas e arquivos para as MVs dentro de um mesmo \textit{data-center} não são realizados.

% Yuan \textit{et al.} \cite{Yuan2010} use a clustering technique \cite{Broder} based on a dependency matrix to identify  data files which will be consumed (\textit{i.e.} shared) by the same tasks in the workflow. The main idea is to store these data files in the same data center to reduce data transfer between different data centers. It is worth noticing that such approach assigns a task to the data center in which most of the input data is stored. The scheduling task problem and data assignment are not deeply treated in this approach, that ignores the scheduling and data assignment in VMs within the same data center. 

Wang \textit{et al.} \cite{Wang2014} apresenta uma solução cujo objetivo é minimizar as transferência de dados em \textit{workflows} alocados a multiplos \textit{data-centers}. O trabalho define uma localização inicial para os dados estáticos utilizando o algoritmo de clusterização k-means \cite{Broder}. Em seguida, uma técnica de replicação de tarefas é utilizada para reduzir as transferências entre diferentes \textit{data-centers} dos dados produzidos durante a execução do \textit{workflow} (dados dinâmicos). Além da complexidade inerente às técnicas de replicação, como garantir a consistência dos dados, a abordagem proposta por Wang \textit{et al.} não trata dos problemas de transferências dentro de um mesmo \textit{data-center}.

%  Wang \textit{et al.} \cite{Wang2014} present a solution whose objective is to minimize the data transfer of workflows allocated in multiple data centers. This work defines the initial data locality  (\textit{i.e.} static data) by using the k-means clustering algorithm \cite{Broder}. Task replication techniques are used to reduce data transfer, which are generated during  the workflow execution (dynamic data), between different data centers. Besides the intrinsic complexity of replication techniques, such as maintenance of data consistency,  the approach proposed by Wang \textit{et al.} does no tackle the data transfer problem within data centers.

Bryk \textit{et al.} \cite{Bryk} propôs um modelo para execução de múltiplos \textit{workflows} em ambientes de nuvem. O algoritmo \textit{File Locality-Aware scheduling} (FLA-S), que se beneficia da localização dos dados para aumentar a performance de execução dos \textit{workflows} é apresentado. O algoritmo executa uma alocação dinâmica de tarefas, na qual, a cada etapa, as tarefas prontas para a execução são escalonadas para as MVs disponíveis. O escalonador prioriza algumas tarefas, considerando a localização dos dados, com o objetivo de minimizar as transferências. O modelo considera que os dados sejam armazenados em um sistema de arquivos centralizado e compartilhado. Porém, cada MV mantém localmente uma cópia dos arquivos que foram gerados nela, isto é, pelas tarefas alocadas à ela. Portanto, embora a alocação dos arquivos não seja definida pelo algoritmo, as tarefas que consomem os mesmo arquivos, ou que apresentam relação de dependência, são preferencialmente alocadas à mesma MV, evitando assim transferências entre as máquinas e o sistema centralizado de arquivos.

% Bryk \textit{et al.} \cite{Bryk} propose a model to execute workflow ensembles in clouds. They present
% the File Locality-Aware Scheduling Algorithm, which benefits of data locality to increase the performance of workflow executions. The algorithm executes a dynamic task scheduling, in which, at each step, the tasks that are ready to be executed are scheduled to available VMs. The scheduler prioritizes some tasks, considering the data locality, aiming at minimizing data transfer (\textit{i.e.} it tries to choose a VM "near" to data). The execution model considers that both storing and data transfer use a global storage and each VM keeps only data files generated in it during the workflow execution. Thus, although the file locality is not defined by the scheduling algorithm, tasks that consume the same file are preferably allocated in the same VM that contains such file, avoiding in these cases data transfer between machines.

Çatalyürek {\it et al.} \cite{Catal2011} apresentam um algoritmo para a alocação de dados e tarefas de \textit{workflow} executados na nuvem. O \textit{workflow} é modelado como um hiper-grafo. Um algoritmo de particionamento é proposto, cujo objetivo é dividir o \textit{workflow} em $k$ partes (onde $k$ é o número de máquinas virtuais do ambiente), e associa cada uma delas a uma MV diferente, e minimiza o número de transferências de dados. Além disso, o particionamento deve atingir um balanceamento previamente definido pelo usuário. Os autores não consideram característica do ambiente, tal como capacidade de processamento e de armazenamento e taxas de transferências. Apenas o tamanho total dos arquivos transferidos é utilizado para avaliar a qualidade da solução.

% Çatalyürek {\it et al.} \cite{Catal2011} presented an algorithm that treats data file  and task allocation of   workflows executed in clouds. The workflow is  modeled as a hypergraph. They propose a  partitioning  algorithm whose objective is dividing    the workflow in k parts (where k is the number of virtual machines), each one to be associated to a different virtual machine, and minimizing the number of file transfers. Besides that, the  partition must meet  the balancing previously defined by the user. Their work does not consider environment characteristics such as processing and storage capacity and transfer rates. Only the total size of all transferred files are used  to evaluate the solution quality.


% To the best of author's knowledge, the approach proposed in this article is the first one that considers both task and data as vertices in the workflow graph. This way, we can consider both problems together by proposing a model and an  algorithm to minimize the makespan of workflows in cloud environments. Our approach considers different storage capacity of machines, different tasks execution times and data transfer times in these heterogeneous systems. The following section formalizes the problem considered in this article.



\begin{table}[!ht]
        \caption{Resumo dos trabalhos relacionados.}
         \label{tab:trabalhos1}
         \resizebox{\textwidth}{!}{\begin{tabular}{|l |c |c |c |c |}
            \hline
            \textbf{Ref.} & \textbf{Algoritmo} & \textbf{Objetivos}             & \textbf{Restrições} & \textbf{Ambiente}\\
            \hline\hline
            Yu e Shi \cite{Yu07}           &  AHEFT             &  \textit{Makespan} &    -         & \textit{Grids}\\ 
            Chopra e Singh \cite{chopra13} &  HEFT-Based        &  Custo financeiro  &  Tempo de execução  & Híbrido\\
            Durillo, Fard e Prodan \cite{durillo12} & MOHEFT    & \textit{Makespan} e  custo financeiro &  -      & Nuvem\\
            Durillo, Nae e Prodan \cite{Durillo14} & MOHEFT     & \textit{Makespan} e  consumo de energia&  -     & \textit{Cluster}\\
            Blythe \textit{et al.} \cite{minmin}   & MinMin     & \textit{Makespan}  &     -       &    \textit{Grids}\\  
            Byun  \textit{et al.} \cite{Byun2011}  & PBTS       & \textit{Makespan}  &    Tempo de execução &  Nuvem\\
            Abrishami \textit{at al.} \cite{Abrishami} & IC-PCP &  Custo financeiro & Tempo de execução     &  Nuvem\\
            Pandey \textit{et al.} \cite{pandey2010}        & PSO &     Custo financeiro &  -               &  Nuvem\\
            Rodriguez e Buyya \cite{Rodriguez2014}          & PSO &     Custo financeiro &  Tempo de execução &  Nuvem\\
            Chen e Zhang \cite{Chen2009}                    & ACO & Vários objetivos             &  Várias restrições &  \textit{Grids}\\
            Hu \textit{et al.} \cite{Hu2010}                & KACO&  -                  & Tempo de execução          & \textit{Grids} \\
            Yu e Buyya \cite{Yu2006}                        & GA  &  -                           & Tempo de execução e custo & Nuvem\\
            Szabo \textit{et al.} \cite{Szabo2013}          & GA  &  \textit{Makespan} e tempo de transferência & - & Nuvem\\
            Yuan \textit{et al.} \cite{Yuan2010}            & \textit{Clustering} & Número de transferências    & - &  Nuvem\\
            Wang \textit{et al.} \cite{Wang2014}           & k-means & Número de transferências &   -    &   Nuvem\\
            Bryk \textit{et al.} \cite{Bryk}               & FLA-S   & Número de transferências &   -    &   Nuvem\\
            Çatalyürek  \textit{et al.} \cite{Catal2011}   & Partition & Número de transferências & -    &   Nuvem\\ 
             \hline 
         \end{tabular}}
\end{table}



