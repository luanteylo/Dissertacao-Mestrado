\chapter{Introdução}\label{chap1}

Avanços recentes na ciência da computação têm permitido que diferentes campos científicos se beneficiem de simulações computacionais. Estudos em dinâmica de fluidos \cite{Guerra2016}, astronomia \cite{Deelman2008}, análises filogenéticas \cite{ocana2011} e bioinformática \cite{livny2008Sipht}, são alguns dos exemplos nos quais os chamados experimentos \textit{in silico} \cite{mattoso2010, Taylor2006} desempenham um papel fundamental no desenvolvimento e na aquisição de novos conhecimentos. Porém, essas aplicações estão produzindo e consumindo um volume inédito de dados, fazendo com que problemas relacionados as limitações computacionais e ao gerenciamento de recursos sejam constantemente enfrentados pelos cientistas \cite{Chen2013}.

Os experimentos científicos são comumente representados como uma cadeia de aplicações, nas quais a saída de um programa é a entrada para outro. Neste contexto, os \textit{workflows} científicos (WfCs) destacam-se como uma solução promissora para elaborar e gerenciar esses experimentos. Um WfC é uma abstração que estrutura as etapas do experimento como um grafo, no qual os nós correspondem às atividades de processamento de dados e as arestas representam os fluxos de dados entre elas \cite{mattoso2010}. Esses \textit{workflows} são gerenciados pelos Sistemas de Gerenciamento de \textit{Workflows} Científicos (SGWfCs), que são utilizados para definir, executar e monitorar essas atividades. Alguns exemplos de SGWfCs são:  Swift/T \cite{Wozniak2013}, Pegasus \cite{deelman2005}, VisTrails \cite{callahan2006}, Apache Taverna \cite{Wolstencroft2013} e Kepler \cite{Altintas2006}.

% % (\textbf{VERIFICAR SER VARREDURA DE PARAMETROS CORRESPONDE A "parameter sweep"})
% Para analisar diferentes resultados de um experimento, um mesmo \textit{workflow} é executado enumeras vezes, usando diferentes arquivos de entrada e/ou diferentes configurações de parâmetros, até que a exploração termine e a análise esteja completa. Essa situação é conhecida na computação paralela como varredura de parâmetros  \cite{Walker2007}. Dessa forma, para explorar o paralelismo de dados, considera-se que cada atividade pode corresponder a várias tarefas executáveis que processam diferentes partes dos dados \cite{Liu2014}. 

Conforme a complexidade dos WfCs cresce, em termos de quantidade de dados processados e tempo de execução, seus requisitos de performance excedem as capacidades dos sistemas sequenciais (computadores pessoais com poucos processadores, por exemplo). Se executados sequencialmente, esses \textit{workflows} podem demorar dias ou até mesmo meses para finalizar, o que não é desejado, devido à propensão a erros de execução, e até mesmo pela competitividade existente no meio científico hoje em dia \cite{Fang2015}. Consequentemente, os ambientes de computação de alto desempenho (HPC, do inglês \textit{High Performance Computing}) aliados às técnicas de paralelismo, tornaram-se essenciais para a obtenção de resultados em tempo aceitável.

Ambientes como \textit{clusters}, super computadores e \textit{grids} computacionais, são tradicionalmente utilizados para a execução de aplicações HPC. No entanto, a computação em nuvem \cite{vaquero2008} vem se destacando como uma opção promissora para a execução dessa classe de aplicações \cite{Liu2015}. A computação em nuvem é um tipo de serviço baseado na Internet na qual infraestrutura computacional virtualmente ilimitada, plataforma e \textit{software} são providos sob demanda, seguindo o modelo de cobrança \textit{pay-per-use} \cite{Youseff08, oliveira2010b}, no qual os usuários são cobrados pelos recursos efetivamente utilizados. Ao contratar esses serviços, a necessidade de aquisição de infraestruturas de alto custo (como \textit{clusters}), e os esforços despendidos com configurações complexas (como ocorre nos \textit{grids}), são substituídos pela aquisição de máquinas virtuais (MVs) pré configuradas e prontas para o uso.  A facilidade em obter recursos computacionais que atendem a diferentes necessidades e custo monetário flexível são alguns dos atrativos que despertam o interesse da comunidade científica por esses ambientes \cite{oliveira2010a}. 

Para permitir a execução de WfCs em ambientes de nuvens computacionais é necessário escalonar cada tarefa  que compõe o \textit{workflow} para uma das MVs disponíveis. Assim, a grosso modo, um algoritmo de escalonamento busca mapear tarefas a recursos, de forma que os requisitos definidos pelo usuário, pelas aplicações ou pelo provedor sejam atendidos \cite{minmin}. O escalonamento de tarefas em recursos distribuídos é um problema NP-Difícil \cite{Ullman1973} e há algumas características das nuvens computacionais que fazem com que esse processo seja ainda mais complicado.

Primeiramente, os serviços de nuvem disponibilizam várias tipos de MVs, cada uma com diferences capacidades de processamento, armazenamento, taxas de transferência e custo financeiro, sendo que algumas dessas MVs não são adequadas para HPC (por exemplo as MVs do tipo micro e nano na Amazon EC2). Além disso, muitos dos \textit{workflows} existentes consomem e produzem vários GB ou mesmo TB de dados, e esses dados (ou pelo menos uma parte deles) são transferidos de uma MV para outra, o que pode impactar o tempo total de execução do \textit{workflow}. Por exemplo, uma única execução do Montage \cite{Deelman2008}, um \textit{workflow} utilizado para gerar mosaicos customizados de imagens astronômicas, pode processar em torno de 200 GB de dados \cite{Juve2013}. Se, durante a execução, esses dados forem transferidos inúmeras vezes entre as MVs, uma parte considerada do tempo total do experimento será gasta em transferência ao invés de processamento (que é o foco do experimento). Sendo assim, ao escalonar as tarefas de um \textit{workflow} é necessário evitar transferências desnecessárias ou, quando a transferência é inevitável, ao menos minimizar o seu impacto no tempo total de execução.

O escalonamento de dados e tarefas em sistemas distribuídos é um tópico largamente discutido nos ambientes de \textit{grids} e \textit{clusters} \cite{Dong2006, HEFT, Isard2009, Ranganathan2002s}. Em \cite{Ranganathan2002s} por exemplo, várias heurísticas de escalonamento foram avaliadas em conjunto com heurística de replicação e movimentação de dados. A avaliação foi feita em um ambiente simulado de \textit{grid} computacional e, segundo os autores, os resultados mostram a importância de tratar o escalonamento de tarefas e dados de forma conjunta. Nos últimos anos, várias abordagens heurísticas para o escalonamento de \textit{workflows} científicos em ambientes de nuvens foram propostas \cite{Liu2014, pandey2010, Yuan2010, Szabo2013, Bryk, Oliveira2012, OliveiraPorto2015}. No entanto, soluções que consideram tanto a distribuição dos dados, quanto a alocação de tarefas foram pouco exploradas.

Neste trabalho, uma nova abordagem para o escalonamento de \textit{workflows} científicos em ambiente de nuvem é apresentada e avaliada. A abordagem considera o escalonamento das tarefas e a distribuição dos dados de forma conjunta, como partes de um mesmo problema. Além disso, para validar a solução, é apresentado um algoritmo evolutivo híbrido \cite{Moscato2010}, que escalona tarefas e dados considerando a heterogeneidade das MVs, as características do ambiente e as restrições impostas aos dados. 

\section{Objetivo e Contribuições do Trabalho}

Este trabalho propõe uma solução para o problema de escalonamento de tarefas e alocação de arquivos de WfCs executados em ambientes de nuvens computacionais. Para isso, um modelo matemático que considera as características das aplicações, do ambiente e a junção entre o escalonamento de tarefas e alocação de arquivos é proposto. Além disso, também é apresentado um algoritmo evolutivo híbrido, que foi avaliado utilizando instâncias de \textit{workflows} sintéticos e reais. O objetivo é minimizar o tempo total de execução (\textit{makespan}) dos \textit{workflows} nestes ambientes e demonstrar a viabilidade da nova abordagem.

Sendo assim, as principais contribuições deste trabalho se resumem nos seguintes pontos:
\begin{enumerate}
    \item Um modelo de representação para \textit{workflows} científicos utilizando grafo, no qual os nós correspondem às atividades de processamento ou arquivos de dados, e os arcos representam as operações de leitura e escrita
    \item Formulação do problema de escalonamento de tarefas e alocação de arquivos de dados como um problema de programação inteira mista, nomeado IP-ETAA
    \item Implementação de um algoritmo evolutivo híbrido para o escalonamento de tarefas e dados em ambientes de nuvem, chamado de AEH-ETAA
    \item Avaliação experimental baseada em \textit{workflows} sintéticos, e em execuções reais utilizando o serviço da Amazon EC2.
\end{enumerate}




\section{Organização do Trabalho}

O trabalho está dividido em 7 capítulos, incluindo a introdução.

O Capítulo \ref{chap2} introduz os conceitos principais dos \textit{workflows} científicos, das nuvens computacionais e do problema de escalonamento. No Capítulo \ref{chap3} os trabalhos relacionados são discutidos. A definição do problema, que inclui o modelo da aplicação, do ambiente e a formulação matemática, é abordada no Capítulo \ref{chap4}.

No Capítulo \ref{chap5}, o algoritmo evolutivo híbrido proposto é apresentado. No Capítulo \ref{chap6}, os resultados das avaliações teóricas e práticas são apresentados e discutidos. Por fim, o Capítulo \ref{chap7} finaliza o trabalho com as conclusões e os trabalhos futuros.



